{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMI0Xiz7RfrkYQFXSQmLlLj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srivarthinivelu/fine_tuning/blob/main/llm_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J9GK81FmJhhE"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(predictions, ground_truth):\n",
        "  correct = 0\n",
        "  for pred, truth in zip(predictions, ground_truth):\n",
        "    if pred == truth:\n",
        "      correct +=1\n",
        "\n",
        "  accuracy = (correct/len(predictions)) * 100\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [\"paris\", \"london\",\"delhi\", \"DC\"]\n",
        "ground_truth = [\"paris\", \"london\", \"delhi\", \"madrid\"]"
      ],
      "metadata": {
        "id": "oiaHIg5pJuKI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc=calculate_accuracy(predictions, ground_truth)\n",
        "print(f\"Accuracy is {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2izhsz8KROa",
        "outputId": "30820017-6dc1-49db-9546-fe717d060e34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is 75.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BLUE Bilungual Evaluation Understudy"
      ],
      "metadata": {
        "id": "zIrWFwcjK4GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu #--sentence_bleu is for 4 gram\n",
        "\n",
        "#Example1 ==> perfect match\n",
        "reference = [[\"the\",\"cat\",\"is\", \"on\", \"the\", \"mat\"]]\n",
        "\n",
        "candidate = [\"the\",\"cat\",\"is\", \"on\", \"the\", \"mat\"]\n",
        "\n",
        "score = sentence_bleu(reference,candidate)\n",
        "print(f\"BLEU score is, {score}\")\n",
        "\n",
        "#Example 2: one word change\n",
        "candidate1 = [\"the\",\"dog\",\"is\", \"on\", \"the\", \"mat\"]\n",
        "score = sentence_bleu(reference,candidate1)\n",
        "print(f\"BLEU score is, {score}\")\n",
        "\n",
        "#Example 3: word order change\n",
        "candidate2 = [\"the\",\"mat\",\"is\", \"on\", \"the\", \"dog\"]\n",
        "score = sentence_bleu(reference,candidate2)\n",
        "print(f\"BLEU score is, {score}\")\n",
        "\n",
        "#Example 4: Negative change\n",
        "candidate3 = [\"the\",\"cat\",\"is\", \"not\", \"on\", \"the\", \"mat\"]\n",
        "score = sentence_bleu(reference,candidate3)\n",
        "print(f\"BLEU score is, {score}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ie26Zb_K_ru",
        "outputId": "a8fa9cac-f298-4c90-b0c0-3e9b27601713"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score is, 1.0\n",
            "BLEU score is, 0.537284965911771\n",
            "BLEU score is, 7.262123179505913e-78\n",
            "BLEU score is, 8.44484326442819e-78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The range of bleu is 0.0 to 1.0\n",
        "1.0 = perfect match\n",
        "0.5 - 0.7 = slight overlap, difference\n",
        "0.0 - 0.3 = poor match\n",
        "~e-78 = zero"
      ],
      "metadata": {
        "id": "JohA5IocVq8G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HfV3mIusO5j4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}